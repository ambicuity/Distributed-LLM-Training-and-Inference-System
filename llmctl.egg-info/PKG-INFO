Metadata-Version: 2.4
Name: llmctl
Version: 0.1.0
Summary: Distributed LLM Training and Inference System
Author-email: LLMCtl Team <team@llmctl.ai>
License: MIT
Project-URL: Homepage, https://github.com/ambicuity/Distributed-LLM-Training-and-Inference-System
Project-URL: Repository, https://github.com/ambicuity/Distributed-LLM-Training-and-Inference-System
Project-URL: Issues, https://github.com/ambicuity/Distributed-LLM-Training-and-Inference-System/issues
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Requires-Python: >=3.8
Description-Content-Type: text/markdown
Requires-Dist: torch>=2.0.0
Requires-Dist: transformers>=4.30.0
Requires-Dist: pydantic>=2.0.0
Requires-Dist: typer>=0.9.0
Requires-Dist: rich>=13.0.0
Requires-Dist: toml>=0.10.0
Requires-Dist: PyYAML>=6.0
Requires-Dist: numpy>=1.21.0
Requires-Dist: psutil>=5.9.0
Requires-Dist: opentelemetry-api>=1.15.0
Requires-Dist: opentelemetry-exporter-otlp>=1.15.0
Requires-Dist: prometheus-client>=0.16.0
Requires-Dist: boto3>=1.26.0
Requires-Dist: datasets>=2.10.0
Requires-Dist: tokenizers>=0.13.0
Requires-Dist: safetensors>=0.3.0
Requires-Dist: accelerate>=0.20.0
Requires-Dist: deepspeed>=0.9.0
Requires-Dist: flask>=2.3.0
Requires-Dist: uvicorn>=0.22.0
Requires-Dist: fastapi>=0.95.0
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Requires-Dist: pytest-cov>=4.0.0; extra == "dev"
Requires-Dist: black>=23.0.0; extra == "dev"
Requires-Dist: isort>=5.12.0; extra == "dev"
Requires-Dist: flake8>=6.0.0; extra == "dev"
Requires-Dist: mypy>=1.3.0; extra == "dev"
Provides-Extra: distributed
Requires-Dist: mpi4py>=3.1.0; extra == "distributed"
Requires-Dist: nccl-pytorch>=2.18.0; extra == "distributed"
Provides-Extra: serving
Requires-Dist: vllm>=0.2.0; extra == "serving"
Requires-Dist: tensorrt>=8.6.0; extra == "serving"
Requires-Dist: onnx>=1.14.0; extra == "serving"
Requires-Dist: onnxruntime>=1.15.0; extra == "serving"

# Distributed LLM Training and Inference System

A comprehensive CLI tool (`llmctl`) that orchestrates data preparation, model partitioning, distributed training, checkpointing, evaluation, and low-latency inference for Large Language Models.

## Features

- **Hardware-aware optimization**: Exposes FLOPs ceilings, memory bandwidth, and communication bandwidth knobs
- **Distributed training**: Pipeline, tensor, and sequence parallelism with ZeRO stages
- **Deterministic & reproducible**: Deterministic training runs with full reproducibility
- **Observability**: Built-in telemetry, metrics collection, and tracing
- **Pluggable backends**: Support for PyTorch/XLA, CUDA/HIP, NCCL/Gloo/MPI
- **Production-ready serving**: Low-latency inference with paged attention and dynamic batching

## Architecture

- **CLI (`llmctl`)**: User experience layer for driving workflows
- **Runtime**: Process orchestration, launchers, schedulers, topology detection
- **Partitioner**: Pipeline+tensor+sequence parallelism planner, sharding planner
- **Kernels/Execution**: Fused kernels, activation checkpointing, custom attention
- **Communications**: Collectives, overlap engine, topology planning
- **IO/Storage**: Dataset streaming, memory mapping, checkpointing
- **Serving**: Inference runtime, KV-cache manager, batching scheduler
- **Metrics**: FLOPs estimation, profiling, OpenTelemetry export
- **Configuration**: Validated schemas, hardware profiles, presets
- **Plugins**: Hardware backends, kernels, schedulers, quantizers

## Quick Start

### Installation

```bash
pip install -e .
```

### Basic Usage

```bash
# Initialize a new project
llmctl init --template gpt --size 7b

# Probe hardware and generate profile
llmctl hw probe --emit configs/hw/local.toml

# Compute parallelism plan
llmctl plan --model gpt-7b.json --hardware configs/hw/local.toml --strategy auto

# Launch distributed training
llmctl train --plan plans/local/7b.toml --data configs/data/dataset.toml

# Evaluate checkpoints
llmctl eval --ckpt checkpoints/step-1000 --suite eval_tasks.toml

# Start inference server
llmctl serve --artifact artifacts/7b-model --port 8080
```

## CLI Commands

| Command | Description |
|---------|-------------|
| `llmctl init` | Scaffold project, create configs and directories |
| `llmctl hw` | Probe hardware, generate hardware profiles |
| `llmctl plan` | Compute parallelism plan given model & hardware constraints |
| `llmctl train` | Launch distributed training with computed plan |
| `llmctl eval` | Evaluate checkpoints (perplexity, accuracy, latency) |
| `llmctl export` | Convert checkpoints to deployment formats |
| `llmctl serve` | Start inference server with batching and paged attention |
| `llmctl bench` | Run micro/macro-benchmarks |
| `llmctl trace` | Capture/visualize runtime traces |
| `llmctl replay` | Deterministically replay a run for debugging |
| `llmctl tune` | Auto-tuning for kernels and communication overlap |
| `llmctl health` | Cluster health checks & drift detection |
| `llmctl admin` | Dataset/index operations, checkpoint GC, tensor inspection |

## Configuration

The system uses TOML configuration files with validated schemas. See `configs/` directory for examples.

## Development

```bash
# Install development dependencies
pip install -e ".[dev]"

# Run tests
pytest tests/

# Format code
black llmctl/
isort llmctl/

# Type checking
mypy llmctl/
```

## License

MIT License
